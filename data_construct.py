"""
ä½¿ç”¨é˜¿é‡ŒQwen-Maxè‡ªåŠ¨æ„å»ºæ–‡æœ¬å†²çªæ•°æ®é›†
åŸºäºé•¿æ–‡æ¡£åˆ†å—ï¼ŒLLMè‡ªåŠ¨ç”Ÿæˆå‰åçŸ›ç›¾çš„è¯­å¥
"""

import json
import re
import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Tuple
import configparser
from openai import OpenAI
import numpy as np
from sentence_transformers import SentenceTransformer
import logging
from datetime import datetime

# å¯¼å…¥é…ç½®ç®¡ç†å™¨å’Œæç¤ºè¯
from config_manager import ConfigManager
from prompts import (
    EXTRACT_STATEMENTS_PROMPT,
    EXTRACT_STATEMENTS_SYSTEM,
    get_conflict_prompt,
    GENERATE_CONFLICT_SYSTEM,
    CATEGORY_DESCRIPTIONS
)

# é…ç½®æ—¥å¿—è®°å½•
def setup_logger(log_dir: str = "logs"):
    """
    é…ç½®æ—¥å¿—è®°å½•å™¨
    
    Args:
        log_dir: æ—¥å¿—æ–‡ä»¶ç›®å½•
    """
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    os.makedirs(log_dir, exist_ok=True)
    
    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å(å¸¦æ—¶é—´æˆ³)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"data_construct_{timestamp}.log")
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info("=" * 80)
    logger.info("æ–‡æœ¬å†²çªæ•°æ®é›†è‡ªåŠ¨æ„å»ºå·¥å…· - æ—¥å¿—è®°å½•å¯åŠ¨")
    logger.info("=" * 80)
    
    return logger

# åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
logger = setup_logger()

# åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨(ä½¿ç”¨æ­£ç¡®çš„é…ç½®æ–‡ä»¶è·¯å¾„)
project_root = Path(__file__).parent
config_manager = ConfigManager(config_path=str(project_root / "config" / "config.cfg"))

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹(ç”¨äºè®¡ç®—ç›¸ä¼¼åº¦)
print("åŠ è½½åµŒå…¥æ¨¡å‹...")
logger.info("å¼€å§‹åŠ è½½åµŒå…¥æ¨¡å‹: sentence-transformers/all-MiniLM-L6-v2")
embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
print("åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")
logger.info("åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")

# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
def calculate_cosine_similarity(text1: str, text2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
    
    Args:
        text1: ç¬¬ä¸€ä¸ªæ–‡æœ¬
        text2: ç¬¬äºŒä¸ªæ–‡æœ¬
    
    Returns:
        float: ä½™å¼¦ç›¸ä¼¼åº¦(0-1ä¹‹é—´)
    """
    logger.debug(f"è®¡ç®—ç›¸ä¼¼åº¦:\n  æ–‡æœ¬1: {text1[:100]}...\n  æ–‡æœ¬2: {text2[:100]}...")
    embeddings = embedding_model.encode([text1, text2])
    similarity = np.dot(embeddings[0], embeddings[1]) / (
        np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])
    )
    logger.debug(f"ç›¸ä¼¼åº¦è®¡ç®—ç»“æœ: {similarity:.4f}")
    return float(similarity)

# åˆå§‹åŒ–Qwenå®¢æˆ·ç«¯
def init_qwen_client():
    """åˆå§‹åŒ–Qwen APIå®¢æˆ·ç«¯"""
    logger.info("åˆå§‹åŒ–Qwen APIå®¢æˆ·ç«¯")
    api_config = config_manager.get_api_config()
    
    if not api_config['api_key']:
        logger.error("APIå¯†é’¥æœªé…ç½®")
        raise ValueError("APIå¯†é’¥æœªé…ç½®ï¼è¯·æ£€æŸ¥ config/config.cfg æ–‡ä»¶ä¸­çš„ ali_api_key é…ç½®")
    
    logger.info(f"API Base URL: {api_config['base_url']}")
    client = OpenAI(
        api_key=api_config['api_key'],
        base_url=api_config['base_url']
    )
    logger.info("Qwen APIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    return client

# è¯»å–å¹¶åˆ†å—æ–‡æ¡£
def load_and_chunk_document(file_path: str, num_chunks: int = 10) -> List[Dict[str, Any]]:
    """
    åŠ è½½æ–‡æ¡£å¹¶åˆ†æˆæŒ‡å®šæ•°é‡çš„å—,ç¡®ä¿ä¸åˆ‡æ–­å¥å­
    
    Args:
        file_path: æ–‡æ¡£è·¯å¾„
        num_chunks: åˆ†å—æ•°é‡
    
    Returns:
        List[Dict]: åŒ…å«chunk_id, textçš„åˆ—è¡¨
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()
    
    # æŒ‰å¥å­åˆ†å‰²(ä¸­æ–‡å¥å·ã€é—®å·ã€æ„Ÿå¹å·ã€è‹±æ–‡å¥å·ç­‰)
    import re
    sentences = re.split(r'([ã€‚!?;;\n]+)', text)
    # å°†åˆ†éš”ç¬¦ä¸å‰é¢çš„å¥å­åˆå¹¶
    full_sentences = []
    for i in range(0, len(sentences)-1, 2):
        if i+1 < len(sentences):
            full_sentences.append(sentences[i] + sentences[i+1])
    if len(sentences) % 2 == 1:  # å¦‚æœæœ€åæœ‰å‰©ä½™
        full_sentences.append(sentences[-1])
    
    # è¿‡æ»¤ç©ºå¥å­
    full_sentences = [s.strip() for s in full_sentences if s.strip()]
    
    total_sentences = len(full_sentences)
    sentences_per_chunk = total_sentences // num_chunks
    
    chunks = []
    
    for i in range(num_chunks):
        start_idx = i * sentences_per_chunk
        end_idx = (i + 1) * sentences_per_chunk if i < num_chunks - 1 else total_sentences
        
        chunk_content = ''.join(full_sentences[start_idx:end_idx])
        
        chunks.append({
            "chunk_id": i,
            "original_text": chunk_content.strip(),
            "processed_text": ""  # åˆå§‹ä¸ºç©º,åç»­ä¼šæ’å…¥å†²çªé™ˆè¿°
        })
    
    print(f"æ–‡æ¡£å·²åˆ†æˆ {len(chunks)} ä¸ªå—(æŒ‰å¥å­è¾¹ç•Œåˆ†å‰²)")
    logger.info(f"æ–‡æ¡£åˆ†å—å®Œæˆ: å…±{len(chunks)}ä¸ªå—, æ€»å¥å­æ•°{total_sentences}")
    for chunk in chunks:
        logger.info(f"  Chunk {chunk['chunk_id']}: {len(chunk['original_text'])}å­—ç¬¦")
    return chunks

# ä¿å­˜åˆ†å—ç»“æœ
def save_chunks(chunks: List[Dict[str, Any]], output_path: str):
    """ä¿å­˜åˆ†å—ç»“æœåˆ°JSONæ–‡ä»¶"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(chunks, f, ensure_ascii=False, indent=4)
    print(f"åˆ†å—ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

# åŠ è½½åˆ†å—ç»“æœ
def load_chunks(chunks_path: str) -> List[Dict[str, Any]]:
    """ä»JSONæ–‡ä»¶åŠ è½½åˆ†å—ç»“æœ"""
    with open(chunks_path, 'r', encoding='utf-8') as f:
        chunks = json.load(f)
    print(f"å·²åŠ è½½ {len(chunks)} ä¸ªæ–‡æœ¬å—")
    return chunks

# å°†å†²çªé™ˆè¿°æ’å…¥åˆ°chunksä¸­
def insert_conflicts_into_chunks(chunks: List[Dict[str, Any]], dataset: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    å°†å†²çªé™ˆè¿°æ’å…¥åˆ°å¯¹åº”chunkçš„éšæœºä½ç½®
    
    Args:
        chunks: åŸå§‹chunkåˆ—è¡¨
        dataset: å†²çªæ•°æ®é›†
    
    Returns:
        List[Dict]: æ›´æ–°åçš„chunkåˆ—è¡¨(åŒ…å«processed_textå­—æ®µ)
    """
    import random
    random.seed(42)
    
    # åˆå§‹åŒ–processed_textä¸ºoriginal_text
    for chunk in chunks:
        if 'original_text' in chunk:
            chunk['processed_text'] = chunk['original_text']
        else:
            chunk['processed_text'] = ""
    
    print(f"\nå°† {len(dataset)} ä¸ªå†²çªé™ˆè¿°æ’å…¥åˆ°å¯¹åº”chunks...")
    
    # æŒ‰chunk_idåˆ†ç»„å†²çªé™ˆè¿°
    conflicts_by_chunk = {}
    for conflict in dataset:
        target_chunk_id = conflict['conflicting_statement']['chunk']
        if target_chunk_id not in conflicts_by_chunk:
            conflicts_by_chunk[target_chunk_id] = []
        conflicts_by_chunk[target_chunk_id].append(conflict['conflicting_statement']['statement'])
    
    # ä¸ºæ¯ä¸ªchunkæ’å…¥å†²çªé™ˆè¿°
    for chunk_id, conflict_statements in conflicts_by_chunk.items():
        if chunk_id >= len(chunks):
            print(f"  âš ï¸ Chunk {chunk_id} è¶…å‡ºèŒƒå›´,è·³è¿‡")
            continue
        
        chunk = chunks[chunk_id]
        text = chunk['processed_text']
        
        if not text:
            print(f"  âš ï¸ Chunk {chunk_id} æ–‡æœ¬ä¸ºç©º,è·³è¿‡")
            continue
        
        # æŒ‰å¥å­åˆ†å‰²
        sentences = re.split(r'([ã€‚!?;;]+)', text)
        full_sentences = []
        for i in range(0, len(sentences)-1, 2):
            if i+1 < len(sentences):
                full_sentences.append(sentences[i] + sentences[i+1])
        if len(sentences) % 2 == 1:
            full_sentences.append(sentences[-1])
        
        full_sentences = [s for s in full_sentences if s.strip()]
        
        if len(full_sentences) < 2:
            print(f"  âš ï¸ Chunk {chunk_id} å¥å­æ•°ä¸è¶³,ç›´æ¥è¿½åŠ ")
            for stmt in conflict_statements:
                chunk['processed_text'] += stmt
            continue
        
        # ä¸ºæ¯ä¸ªå†²çªé™ˆè¿°éšæœºé€‰æ‹©æ’å…¥ä½ç½®
        for stmt in conflict_statements:
            # éšæœºé€‰æ‹©ä¸¤ä¸ªç›¸é‚»å¥å­ä¹‹é—´çš„ä½ç½®(1åˆ°len-1ä¹‹é—´)
            insert_pos = random.randint(1, len(full_sentences))
            full_sentences.insert(insert_pos, stmt)
            print(f"  Chunk {chunk_id}: åœ¨ä½ç½® {insert_pos} æ’å…¥å†²çªé™ˆè¿°")
        
        # é‡æ–°ç»„åˆæ–‡æœ¬
        chunk['processed_text'] = ''.join(full_sentences)
    
    print(f"å†²çªé™ˆè¿°æ’å…¥å®Œæˆ")
    return chunks

# ä½¿ç”¨LLMæå–å…³é”®é™ˆè¿°
def extract_key_statements(client: OpenAI, chunk_text: str, chunk_id: int, num_statements: int = 3) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨LLMä»æ–‡æœ¬å—ä¸­æå–å…³é”®é™ˆè¿°
    
    Args:
        client: OpenAIå®¢æˆ·ç«¯
        chunk_text: æ–‡æœ¬å—å†…å®¹
        chunk_id: å—ID
        num_statements: è¦æå–çš„é™ˆè¿°æ•°é‡
    
    Returns:
        List[Dict]: åŒ…å«statementå’Œpositionçš„åˆ—è¡¨
    """
    logger.info(f"\n{'='*60}")
    logger.info(f"æå–å…³é”®é™ˆè¿° - Chunk {chunk_id}")
    logger.info(f"{'='*60}")
    
    prompt = EXTRACT_STATEMENTS_PROMPT.format(
        num_statements=num_statements,
        chunk_text=chunk_text[:2000]
    )
    
    logger.info(f"ç³»ç»Ÿæç¤ºè¯:\n{EXTRACT_STATEMENTS_SYSTEM}")
    logger.info(f"\nç”¨æˆ·æç¤ºè¯:\n{prompt}")

    try:
        logger.info(f"\nå‘é€APIè¯·æ±‚: model=qwen-max-latest, temperature=0.1, max_tokens=1000")
        response = client.chat.completions.create(
            model="qwen-max-latest",
            messages=[
                {"role": "system", "content": EXTRACT_STATEMENTS_SYSTEM},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,  # é™ä½æ¸©åº¦ä»¥æé«˜å‡†ç¡®æ€§
            max_tokens=1000
        )
        
        result = response.choices[0].message.content
        logger.info(f"\nLLMè¿”å›å†…å®¹:\n{result}")
        
        # æå–JSON
        json_match = re.search(r'\{.*\}', result, re.DOTALL)
        if json_match:
            data = json.loads(json_match.group())
            statements = data.get("statements", [])
            
            # æ·»åŠ chunk_id
            for stmt in statements:
                stmt["chunk"] = chunk_id
            
            logger.info(f"\næˆåŠŸæå– {len(statements)} æ¡é™ˆè¿°:")
            for i, stmt in enumerate(statements, 1):
                logger.info(f"  {i}. {stmt['statement']}")
            
            return statements
        else:
            logger.warning(f"Chunk {chunk_id} æœªèƒ½è§£æJSONå“åº”")
            print(f"âš ï¸ Chunk {chunk_id} æœªèƒ½è§£æJSONå“åº”")
            return []
            
    except Exception as e:
        logger.error(f"Chunk {chunk_id} æå–é™ˆè¿°å¤±è´¥: {e}", exc_info=True)
        print(f"âŒ Chunk {chunk_id} æå–é™ˆè¿°å¤±è´¥: {e}")
        return []

# ç”Ÿæˆå†²çªé™ˆè¿°
def generate_conflicting_statement(client: OpenAI, original_stmt: str, category: int, 
                                   target_chunk_id: int, chunk_distance: int) -> Dict[str, Any]:
    """
    ä½¿ç”¨LLMç”Ÿæˆä¸åŸé™ˆè¿°å†²çªçš„è¯­å¥
    
    Args:
        client: OpenAIå®¢æˆ·ç«¯
        original_stmt: åŸå§‹é™ˆè¿°
        category: å†²çªç±»å‹ (1=æ•°å€¼å†²çª, 2=è¯­ä¹‰å†²çª, 3=é€»è¾‘å†²çª)
        target_chunk_id: ç›®æ ‡å—ID
        chunk_distance: å—é—´è·ç¦»
    
    Returns:
        Dict: å†²çªé™ˆè¿°ä¿¡æ¯
    """
    logger.info(f"\n{'='*60}")
    logger.info(f"ç”Ÿæˆå†²çªé™ˆè¿° - ç±»å‹{category} ({CATEGORY_DESCRIPTIONS.get(category, 'æœªçŸ¥')})")
    logger.info(f"{'='*60}")
    logger.info(f"åŸå§‹é™ˆè¿°: {original_stmt}")
    logger.info(f"ç›®æ ‡Chunk: {target_chunk_id}, å—è·ç¦»: {chunk_distance}")
    
    # æ ¹æ®ç±»å‹è·å–å¯¹åº”çš„æç¤ºè¯
    prompt = get_conflict_prompt(category, original_stmt)
    
    logger.info(f"\nç³»ç»Ÿæç¤ºè¯:\n{GENERATE_CONFLICT_SYSTEM}")
    logger.info(f"\nç”¨æˆ·æç¤ºè¯:\n{prompt}")

    try:
        logger.info(f"\nå‘é€APIè¯·æ±‚: model=qwen-max-latest, temperature=0.7, max_tokens=500")
        response = client.chat.completions.create(
            model="qwen-max-latest",
            messages=[
                {"role": "system", "content": GENERATE_CONFLICT_SYSTEM},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=500
        )
        
        result = response.choices[0].message.content
        logger.info(f"\nLLMè¿”å›å†…å®¹:\n{result}")
        
        # æå–JSON
        json_match = re.search(r'\{.*\}', result, re.DOTALL)
        if json_match:
            data = json.loads(json_match.group())
            
            conflict_info = {
                "statement": data.get("conflicting_statement", ""),
                "chunk": target_chunk_id,
                "position": -1,  # LLMç”Ÿæˆçš„,æ— å…·ä½“ä½ç½®
                "contradiction_level": data.get("contradiction_level", 0)
            }
            
            logger.info(f"\nç”Ÿæˆçš„å†²çªé™ˆè¿°: {conflict_info['statement']}")
            logger.info(f"çŸ›ç›¾å±‚çº§: {conflict_info['contradiction_level']}")
            
            return conflict_info
        else:
            logger.warning("æœªèƒ½è§£æå†²çªé™ˆè¿°çš„JSONå“åº”")
            print(f"âš ï¸ æœªèƒ½è§£æå†²çªé™ˆè¿°çš„JSONå“åº”")
            return None
            
    except Exception as e:
        logger.error(f"ç”Ÿæˆå†²çªé™ˆè¿°å¤±è´¥: {e}", exc_info=True)
        print(f"âŒ ç”Ÿæˆå†²çªé™ˆè¿°å¤±è´¥: {e}")
        return None

# æ„å»ºæ•°æ®é›†
def build_conflict_dataset(client: OpenAI, chunks: List[Dict[str, Any]], 
                          num_conflicts: int = 10, 
                          min_chunk_distance: int = 3) -> List[Dict[str, Any]]:
    """
    æ„å»ºå®Œæ•´çš„å†²çªæ•°æ®é›†
    
    Args:
        client: OpenAIå®¢æˆ·ç«¯
        chunks: æ–‡æ¡£å—åˆ—è¡¨
        num_conflicts: è¦ç”Ÿæˆçš„å†²çªæ•°é‡
        min_chunk_distance: æœ€å°å—é—´è·ç¦»
    
    Returns:
        List[Dict]: å†²çªæ•°æ®é›†
    """
    import random
    import math
    random.seed(42)
    
    dataset = []
    
    # è®¡ç®—æ¯ä¸ªchunkåº”è¯¥æå–çš„é™ˆè¿°æ•°é‡(å‘ä¸Šå–æ•´)
    num_valid_chunks = len(chunks) - 1
    statements_per_chunk = math.ceil(num_conflicts / num_valid_chunks)
    
    print(f"\nä» {len(chunks)} ä¸ªå—ä¸­æå–å…³é”®é™ˆè¿°...")
    print(f"  æœ‰æ•ˆchunkæ•°: {num_valid_chunks} (æ’é™¤æœ€åä¸€ä¸ª)")
    print(f"  æ¯ä¸ªchunkæå–: {statements_per_chunk} æ¡é™ˆè¿°")
    logger.info(f"\nå¼€å§‹æ„å»ºå†²çªæ•°æ®é›†: {len(chunks)}ä¸ªå—, ç›®æ ‡{num_conflicts}ä¸ªå†²çª")
    logger.info(f"æœ‰æ•ˆchunkæ•°: {num_valid_chunks}, æ¯ä¸ªchunkæå–{statements_per_chunk}æ¡é™ˆè¿°")
    
    # ç¬¬ä¸€æ­¥ï¼šä»æ‰€æœ‰å—ä¸­æå–å…³é”®é™ˆè¿°(æ’é™¤æœ€åä¸€ä¸ªchunk,å› ä¸ºæ²¡æœ‰åç»­chunkå¯æ’å…¥å†²çª)
    all_statements = []
    for chunk in chunks[:-1]:  # æ’é™¤æœ€åä¸€ä¸ªchunk
        print(f"  å¤„ç† Chunk {chunk['chunk_id']}...")
        logger.info(f"\nå¤„ç† Chunk {chunk['chunk_id']}")
        # ä»original_textå­—æ®µè¯»å–åŸå§‹æ–‡æœ¬
        chunk_text = chunk.get('original_text', chunk.get('text', ''))
        statements = extract_key_statements(client, chunk_text, chunk['chunk_id'], num_statements=statements_per_chunk)
        all_statements.extend(statements)
    
    print(f"å…±æå– {len(all_statements)} æ¡å…³é”®é™ˆè¿°(ä»å‰{len(chunks)-1}ä¸ªchunk)")
    logger.info(f"\næ€»è®¡æå– {len(all_statements)} æ¡å…³é”®é™ˆè¿°(æ’é™¤æœ€åä¸€ä¸ªchunk)")
    
    # ç¬¬äºŒæ­¥ï¼šéšæœºé€‰æ‹©é™ˆè¿°å¹¶ç”Ÿæˆå†²çª
    print(f"\nç”Ÿæˆ {num_conflicts} ä¸ªå†²çªå¯¹...")
    logger.info(f"\nå¼€å§‹ç”Ÿæˆ {num_conflicts} ä¸ªå†²çªå¯¹")
    
    selected_statements = random.sample(all_statements, min(num_conflicts, len(all_statements)))
    
    for idx, original_stmt_info in enumerate(selected_statements, 1):
        print(f"\n  [{idx}/{num_conflicts}] å¤„ç†é™ˆè¿°: {original_stmt_info['statement'][:50]}...")
        logger.info(f"\n\n{'#'*80}")
        logger.info(f"å†²çªå¯¹ {idx}/{num_conflicts}")
        logger.info(f"{'#'*80}")
        
        original_chunk = original_stmt_info['chunk']
        
        # é¦–å…ˆå°è¯•é€‰æ‹©è·ç¦»è¶³å¤Ÿè¿œä¸”åœ¨åŸå§‹chunkä¹‹åçš„ç›®æ ‡å—
        available_chunks = [c for c in range(original_chunk + min_chunk_distance, len(chunks))]
        
        # å¦‚æœæ²¡æœ‰æ»¡è¶³min_chunk_distanceçš„å—,åˆ™ä»æ‰€æœ‰åç»­å—ä¸­é€‰æ‹©
        if not available_chunks:
            available_chunks = [c for c in range(original_chunk + 1, len(chunks))]
            if not available_chunks:
                print(f"    âš ï¸ æ²¡æœ‰åç»­å—å¯ç”¨ï¼Œè·³è¿‡")
                logger.warning(f"Chunk {original_chunk} æ²¡æœ‰åç»­å—å¯ç”¨ï¼Œè·³è¿‡è¯¥å†²çªå¯¹")
                continue
            print(f"    â„¹ï¸ æ— æ³•æ»¡è¶³æœ€å°è·ç¦»{min_chunk_distance},ä»å‰©ä½™{len(available_chunks)}ä¸ªåç»­å—ä¸­é€‰æ‹©")
            logger.info(f"æ— æ³•æ»¡è¶³æœ€å°è·ç¦»{min_chunk_distance},ä»{len(available_chunks)}ä¸ªåç»­å—ä¸­é€‰æ‹©")
        
        target_chunk = random.choice(available_chunks)
        chunk_distance = target_chunk - original_chunk
        
        # éšæœºé€‰æ‹©å†²çªç±»å‹
        category = random.choice([1, 2, 3])
        logger.info(f"é€‰æ‹©ç›®æ ‡Chunk: {target_chunk}, å—è·ç¦»: {chunk_distance}")
        logger.info(f"å†²çªç±»å‹: {category} ({CATEGORY_DESCRIPTIONS.get(category, 'æœªçŸ¥')})")
        
        # ç”Ÿæˆå†²çªé™ˆè¿°
        conflicting_info = generate_conflicting_statement(
            client, 
            original_stmt_info['statement'],
            category,
            target_chunk,
            chunk_distance
        )
        
        if conflicting_info:
            # è®¡ç®—å®é™…çš„ä½™å¼¦ç›¸ä¼¼åº¦
            logger.info(f"\nè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦")
            actual_similarity = calculate_cosine_similarity(
                original_stmt_info['statement'],
                conflicting_info['statement']
            )
            
            conflict_record = {
                "category": category,
                "original_statement": {
                    "statement": original_stmt_info['statement'],
                    "chunk": original_chunk,
                    "position": original_stmt_info.get('position', 0)
                },
                "conflicting_statement": conflicting_info,
                "chunk_distance": chunk_distance,
                "similarity": round(actual_similarity, 4),  # ä½¿ç”¨å®é™…è®¡ç®—çš„ä½™å¼¦ç›¸ä¼¼åº¦
                "contradiction_level": conflicting_info['contradiction_level'],
                "source": "auto"
            }
            
            dataset.append(conflict_record)
            print(f"    ç”Ÿæˆå†²çªå¯¹ (ç±»å‹{category}, è·ç¦»{chunk_distance}å—, ç›¸ä¼¼åº¦{actual_similarity:.4f})")
            logger.info(f"\nå†²çªå¯¹ç”ŸæˆæˆåŠŸ:")
            logger.info(f"  ç±»å‹: {category}")
            logger.info(f"  å—è·ç¦»: {chunk_distance}")
            logger.info(f"  ä½™å¼¦ç›¸ä¼¼åº¦: {actual_similarity:.4f}")
            logger.info(f"  çŸ›ç›¾å±‚çº§: {conflicting_info['contradiction_level']}")
        else:
            print(f"    âŒ å†²çªç”Ÿæˆå¤±è´¥")
            logger.error(f"å†²çªå¯¹ {idx} ç”Ÿæˆå¤±è´¥")
    
    print(f"\næˆåŠŸç”Ÿæˆ {len(dataset)} ä¸ªå†²çªå¯¹")
    logger.info(f"\nå†²çªæ•°æ®é›†æ„å»ºå®Œæˆ: æˆåŠŸç”Ÿæˆ {len(dataset)} ä¸ªå†²çªå¯¹")
    return dataset

# ä¿å­˜æ•°æ®é›†
def save_dataset(dataset: List[Dict[str, Any]], output_path: str):
    """ä¿å­˜æ•°æ®é›†åˆ°JSONæ–‡ä»¶"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(dataset, f, ensure_ascii=False, indent=4)
    print(f"æ•°æ®é›†å·²ä¿å­˜åˆ°: {output_path}")

# ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
def generate_statistics(dataset: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ç”Ÿæˆæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    stats = {
        "total_conflicts": len(dataset),
        "category_distribution": {},
        "contradiction_level_distribution": {},
        "avg_chunk_distance": 0,
        "avg_similarity": 0,
        "source_distribution": {}
    }
    
    for record in dataset:
        # ç±»åˆ«åˆ†å¸ƒ
        cat = record['category']
        stats['category_distribution'][cat] = stats['category_distribution'].get(cat, 0) + 1
        
        # çŸ›ç›¾å±‚çº§åˆ†å¸ƒ
        level = record['contradiction_level']
        stats['contradiction_level_distribution'][level] = \
            stats['contradiction_level_distribution'].get(level, 0) + 1
        
        # æ¥æºåˆ†å¸ƒ
        source = record['source']
        stats['source_distribution'][source] = stats['source_distribution'].get(source, 0) + 1
        
        # ç´¯åŠ ç”¨äºè®¡ç®—å¹³å‡å€¼
        stats['avg_chunk_distance'] += record['chunk_distance']
        stats['avg_similarity'] += record['similarity']
    
    # è®¡ç®—å¹³å‡å€¼
    if dataset:
        stats['avg_chunk_distance'] /= len(dataset)
        stats['avg_similarity'] /= len(dataset)
    
    return stats

# ä¸»å‡½æ•°
def main():
    """ä¸»å‡½æ•°"""
    print("æ–‡æœ¬å†²çªæ•°æ®é›†è‡ªåŠ¨æ„å»ºå·¥å…·")
    print("=" * 60)
    
    # é…ç½®å‚æ•°
    input_file = "dataset/data.txt"
    chunks_file = "dataset/chunks.json"
    output_file = "dataset/error_data1.json"
    num_chunks = 10
    num_conflicts = 50
    min_chunk_distance = 3
    
    logger.info("\nç¨‹åºå‚æ•°é…ç½®:")
    logger.info(f"  è¾“å…¥æ–‡ä»¶: {input_file}")
    logger.info(f"  åˆ†å—æ–‡ä»¶: {chunks_file}")
    logger.info(f"  è¾“å‡ºæ–‡ä»¶: {output_file}")
    logger.info(f"  åˆ†å—æ•°é‡: {num_chunks}")
    logger.info(f"  å†²çªæ•°é‡: {num_conflicts}")
    logger.info(f"  æœ€å°å—è·ç¦»: {min_chunk_distance}")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(input_file):
        print(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return
    
    # åˆå§‹åŒ–Qwenå®¢æˆ·ç«¯
    print("\nåˆå§‹åŒ–Qwen APIå®¢æˆ·ç«¯...")
    client = init_qwen_client()
    print("å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    
    # æ­¥éª¤1: åŠ è½½å¹¶åˆ†å—æ–‡æ¡£,ä¿å­˜åˆ°chunks.json
    print(f"\næ­¥éª¤1: åŠ è½½æ–‡æ¡£å¹¶åˆ†å—")
    print(f"  è¾“å…¥æ–‡ä»¶: {input_file}")
    logger.info("\n" + "="*80)
    logger.info("æ­¥éª¤1: åŠ è½½æ–‡æ¡£å¹¶åˆ†å—")
    logger.info("="*80)
    chunks = load_and_chunk_document(input_file, num_chunks)
    save_chunks(chunks, chunks_file)
    logger.info(f"åˆ†å—ç»“æœå·²ä¿å­˜: {chunks_file}")
    
    # print(f"\nğŸ“Š åˆ†å—ç»Ÿè®¡:")
    # for chunk in chunks:
    #     print(f"  Chunk {chunk['chunk_id']}: {len(chunk['text'])} å­—ç¬¦")
    
    # æ­¥éª¤2: ä»chunks.jsonè¯»å–åˆ†å—,æ„å»ºå†²çªæ•°æ®é›†
    print(f"\næ­¥éª¤2: ä»åˆ†å—æ–‡ä»¶è¯»å–å¹¶æ„å»ºå†²çªæ•°æ®é›†")
    logger.info("\n" + "="*80)
    logger.info("æ­¥éª¤2: æ„å»ºå†²çªæ•°æ®é›†")
    logger.info("="*80)
    chunks = load_chunks(chunks_file)
    
    dataset = build_conflict_dataset(
        client, 
        chunks, 
        num_conflicts=num_conflicts,
        min_chunk_distance=min_chunk_distance
    )
    
    # ä¿å­˜æ•°æ®é›†
    print(f"\næ­¥éª¤3: ä¿å­˜å†²çªæ•°æ®é›†...")
    logger.info("\n" + "="*80)
    logger.info("æ­¥éª¤3: ä¿å­˜å†²çªæ•°æ®é›†")
    logger.info("="*80)
    save_dataset(dataset, output_file)
    logger.info(f"å†²çªæ•°æ®é›†å·²ä¿å­˜: {output_file}")
    
    # æ­¥éª¤4: å°†å†²çªé™ˆè¿°æ’å…¥åˆ°chunkså¹¶æ›´æ–°chunks.json
    print(f"\næ­¥éª¤4: å°†å†²çªé™ˆè¿°æ’å…¥åˆ°chunks")
    logger.info("\n" + "="*80)
    logger.info("æ­¥éª¤4: æ’å…¥å†²çªé™ˆè¿°åˆ°chunks")
    logger.info("="*80)
    chunks = insert_conflicts_into_chunks(chunks, dataset)
    save_chunks(chunks, chunks_file)
    logger.info(f"æ›´æ–°åçš„åˆ†å—æ–‡ä»¶å·²ä¿å­˜: {chunks_file}")
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    print(f"\næ•°æ®é›†ç»Ÿè®¡:")
    logger.info("\n" + "="*80)
    logger.info("æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯")
    logger.info("="*80)
    stats = generate_statistics(dataset)
    print(f"  æ€»å†²çªæ•°: {stats['total_conflicts']}")
    print(f"  ç±»åˆ«åˆ†å¸ƒ: {stats['category_distribution']}")
    print(f"  çŸ›ç›¾å±‚çº§: {stats['contradiction_level_distribution']}")
    print(f"  å¹³å‡å—è·ç¦»: {stats['avg_chunk_distance']:.2f}")
    print(f"  å¹³å‡ç›¸ä¼¼åº¦: {stats['avg_similarity']:.2f}")
    print(f"  æ¥æºåˆ†å¸ƒ: {stats['source_distribution']}")
    
    logger.info(f"æ€»å†²çªæ•°: {stats['total_conflicts']}")
    logger.info(f"ç±»åˆ«åˆ†å¸ƒ: {stats['category_distribution']}")
    logger.info(f"çŸ›ç›¾å±‚çº§åˆ†å¸ƒ: {stats['contradiction_level_distribution']}")
    logger.info(f"å¹³å‡å—è·ç¦»: {stats['avg_chunk_distance']:.2f}")
    logger.info(f"å¹³å‡ç›¸ä¼¼åº¦: {stats['avg_similarity']:.2f}")
    logger.info(f"æ¥æºåˆ†å¸ƒ: {stats['source_distribution']}")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats_file = output_file.replace('.json', '_stats.json')
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=4)
    print(f"\nç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
    logger.info(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_file}")
    
    print("\næ•°æ®é›†æ„å»ºå®Œæˆï¼")
    print(f"  - åˆ†å—æ–‡ä»¶(å«æ’å…¥çš„å†²çª): {chunks_file}")
    print(f"  - å†²çªæ•°æ®é›†: {output_file}")
    print(f"  - ç»Ÿè®¡ä¿¡æ¯: {stats_file}")
    
    logger.info("\n" + "="*80)
    logger.info("æ•°æ®é›†æ„å»ºå®Œæˆ")
    logger.info("="*80)
    logger.info(f"åˆ†å—æ–‡ä»¶: {chunks_file}")
    logger.info(f"å†²çªæ•°æ®é›†: {output_file}")
    logger.info(f"ç»Ÿè®¡ä¿¡æ¯: {stats_file}")
    logger.info(f"æ—¥å¿—æ–‡ä»¶: logs/data_construct_*.log")

if __name__ == "__main__":
    main()
